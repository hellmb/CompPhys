\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{tensor}
\usepackage{subfigure}
\usepackage{float}
\usepackage{natbib}
\usepackage{url}
%Include packages to have python(or other prog. languages) scripts included in the pdf-file without broken lines
\usepackage{color}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
tabsize=2}
\lstset{style=mystyle}
%Making vectors bold instead of putting arrow on it
\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center

\textsc{\LARGE University of Oslo}\\[1.5cm]
\textsc{\Large FYS4150}\\[0.5cm]
\textsc{\large Kristine Moseid, Helene Aune og Helle Bakke}\\[0.5cm]

\begin{minipage}{0.4\textwidth}
\end{minipage}\\[1cm]

\HRule \\[0.4cm]
{ \huge \bfseries Introduction to numerical projects}\\[0.4cm]
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\end{minipage}\\[8cm]


{\large \today}\\[3cm]
\vfill

\end{titlepage}

\newpage
\tableofcontents

\newpage

\section{Abstract}

\begin{itemize}
\item Tease the reader
\item Write last
\end{itemize}

\section{Introduction}

\begin{itemize}
\item Motivate the reader
\item What have we done
\item Structure of report
\end{itemize}

\noindent The aim of this project is to solve the one-dimensional Poisson equation with Dirichlet boundary conditions by rewriting it as a set of linear equations. We will be solving the equation
\begin{align*}
\frac{d^2\phi}{dr^r} = -4 \pi r \rho(r)
\end{align*}
\noindent By letting $\phi \rightarrow u$ and $r \rightarrow x$ it is simplified to
\begin{align*}
-u^{\prime \prime}(x) = f(x), \quad x \in (0,1), \quad  u(0) = u(1) = 0
\end{align*} 
where we define the discretized approximation to $u$ as $v_i$ with grid point $x_i = ih$ in the interval from $x_0$ to $x_{n+1} = 1$, and the step length as $h = 1/(n+1)$. \\

\noindent By doing this we will be able to create algorithms for solving the tridiagonal matrix problem, and find out how efficient this is compared to other matrix elimination methods. 


\section{Methods}

\begin{itemize}
\item Describe methods and algorithms
\item Explain
\item Calculations to demonstrate the code, verify results(benchmarks)
\end{itemize}

\subsection{Tridiagonal matrix}

\noindent With the bounadry condition $v_0 = v_{n+1} = 0$, the approximation of the second derivative of $u$ was written as 

\begin{align*}
- \frac{v_{i+1} + v_{i-1} - 2v_i}{h^2} = f_i, \quad i = 1,...,n
\intertext{where $f_i = f(x)$. We then rewrote the equation as a linear set of equations:}
- (v_{i+1} + v_{i-1} - 2v_i) = h^2f_i
\intertext{We set $h^2f_i = d_i$, and solved this equation for a few values of $i$.}
\intertext{$i = 1$:}
- (v_{1+1} + v_{1-1} - 2v_1) &= d_1 \\
- (v_2 + v_0 - 2v_1) &= d_1 \\
- v_2 - 0 + 2v_1 &= d_1
\intertext{$i = 2$:}
- (v_{2+1} + v_{2-1} - 2v_2) &= d_2 \\
- v_3 - v_1 + 2v_2) &= d_2
\intertext{$i = 3$:}
- (v_{3+1} + v_{3-1} - 2v_3) &= d_3 \\
- v_4 - v_2 + 2v_3) &= d_3	
\end{align*}
\begin{align*}
\intertext{We saw that this could be written as a linear set of equations $\vec{A} \vec{v} = \vec{d}$,}
 \left(\begin{array}{cccccc}
 	2& -1& 0 &\dots   & \dots &0 \\
     -1 & 2 & -1 &0 &\dots &\dots \\
     0&-1 &2 & -1 & 0 & \dots \\
     & \dots   & \dots &\dots   &\dots & \dots \\
     0&\dots   &  &-1 &2& -1 \\
     0&\dots    &  & 0  &-1 & 2 \\
     \end{array} \right)
\left(\begin{array}{c}
	v_1 \\
	v_2 \\
	v_3 \\
	\dots \\
	v_{n-1} \\
	v_n \\
	\end{array} \right) =
\left(\begin{array}{c}
	d_1 \\
	d_2 \\
	d_3 \\
	\dots \\
	d_{n-1} \\
	d_n \\
	\end{array} \right)
\end{align*}


\subsection{Relative error}
\noindent We computed the relative error in the data set $i = 1,...,n$ by using the expression
\begin{align*}
\epsilon_i = log_{10} \Big( \Big| \frac{v_i - u_i}{u_i}  \Big| \Big)
\end{align*}
\noindent We implemented the cosed-form solution $u(x) = 1 - (1 - e^{-10})x - e^{-10x}$ to our code and calculated the relative error when increasing $n$ to $n = 10^7$. 


\subsection{LU decomposition}

\noindent In this exercise we used the C++ library \textit{'armadillo'} to solve the LU decomposition for matrices of size $10 \times 10$, $100 \times 100$ and $1000 \times 1000$. We implemented the linear equation $$\vec{A} = \vec{L} \vec{u}$$ and used
\begin{lstlisting}
    // find the LU decomposition
    lu(L,U,A);
\end{lstlisting}
to solve it.\\
\bigskip
\noindent We also tried to run the code for a matrix of size $10^5 \times 10^5$.


\section{Results}

\begin{itemize}
\item Present results
\item Critical discussion
\item Put code etc. on GitHub and explain to reader where they can find it
\item Explanatory figures with captions, labels etc.
\end{itemize}

\subsection{Tridiagonal algorithm for special case}
\begin{center}
\begin{tabular}{ c | c | c }
	$n$ & CPU time general & CPU time special \\
	\hline
	10 & 4e-6 & 2e-6 \\
	$10^2$ & 8e-6 & 6e-6 \\
	$10^3$ & 6.7e-5 & 4.3e-5 \\
	$10^4$ & 6.08e-4 & 4.79e-4 \\
	$10^5$ & 0.004186 & 0.002699 \\
	$10^6$ & 0.037559 & 0.22339 \\
\end{tabular}
\medskip
\\
Table 4.1 CPU time for general and special algorithm
\end{center}

\noindent We saw that the special algortihm was a bit faster than the general. \\


\subsection{Relative error}
\noindent In our python-code we took the minimum value of each list of error values. This was because the error values became negative, as a result of $u(x)$ being exponetial. We created a table of the relative error results:

\begin{center}
\begin{tabular}{ c | c }
	$n$ & $\epsilon$ \\
	\hline
	10 & -1.1797 \\
	$10^2$ & -3.08804 \\
	$10^3$ & -5.08005 \\
	$10^4$ & -7.07936 \\
	$10^5$ & -9.0049 \\
	$10^6$ & -6.77137 \\
	$10^7$ & -12.8074\\ 
\end{tabular}
\medskip
\\
Table 4.2 Table of the relative error $\epsilon$ for increasing $n$
\end{center}

\noindent We saw that the error became smaller when $n$ increased, but for $n = 10^6$ this was not the case. Why?

\subsection{LU decomposition}

\noindent The number of floating point operations for the LU decomposition is $\frac{2}{3}n^3$\citep{LU}. We noticed that the program used some time to execute the command. The matrix of size $10^5 \times 10^5$ did not execute. This was because the memory needed to create the matrix was too big for our computers. We calculated how much memory was needed to create the matrix:
\begin{align*}
    \frac{10^5 \cdot 10^5 \cdot 64}{8} &= \frac{8 \cdot 10^{10}}{1024} \enskip \text{B} \\
    &= \frac{78125000}{1024} \enskip \text{kB} \\
    &= \frac{762939}{1024} \enskip \text{MB} \\
    &= 74.5 \enskip \text{GB}
\end{align*}
\noindent It was with good reason that our computers where not able to execute the program for this matrix size.

\section{Conclusion}

\begin{itemize}
\item Main findings
\item Perspectives on improvement and future work
\end{itemize}

\noindent Although it may be tempting to use \textit{'armadillo'} to solve tridiagonal matrix decompositions, our results showed that it was more efficient to create algorithms to solve the problem. The CPU time for forward and backward substitution was shorter than for the LU decomposition. The relative error when solving the algorithms became smaller for larger values of $n$. However, the figures showed us that we did not need very large values of $n$ to get a good approximation to the exact function. Calculating the algorithms for large vectors took a longer time, and we learned that we did not have to do this to get a good result. 


\section{Appendix}

\begin{itemize}
\item Additional calulations
\item Selected calulations with comments
\item Code, if necessary
\item Appendix can be pushed to GitHub!
\end{itemize}

\noindent To run the programs, one needs to give an input argument. This argument varies between $1,...,7$, and refers to the exponent of $10$ for each $n$-value. This applies to the programs \textit{'project1\_main.cpp'} and \textit{'project1\_c.cpp'}. \\

\noindent The program \textit{'project1\_main.cpp'} contains the algorithms calculated in exercise 1b, the calculation of the relative error $\epsilon_i$ in exercise 1d and the calculation of CPU time. The program \textit{'project1\_c.cpp'} contains the specialized algorithm, and the QT folder contains the LU decomposition using \textit{'armadillo'}. \\

\noindent The program \textit{'project1\_python.py'} contains the plotting of figures in exercise 1b, as well as a calculation of the maximum value in each error list. All text-files are created in the main C++ program, and imported into the Python program for further computations. 

\section{References}

\begin{itemize}
\item Reference to material we based our work on(lecture notes etc.)
\item Find scientific articles, books etc.
\item BibTex - extract references online
\end{itemize}



\end{document}
